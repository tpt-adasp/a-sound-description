{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cec52a7",
   "metadata": {},
   "source": [
    "**(CD_All for ESC50 dataset)**\n",
    "# Audio Classification with Sound Descriptions\n",
    "\n",
    "This notebook reproduces results for the ESC-50 and FSD50K datasets shown in the paper **\"A SOUND DESCRIPTION: EXPLORING PROMPT TEMPLATES AND CLASS DESCRIPTIONS TO ENHANCE ZERO-SHOT AUDIO CLASSIFICATION\"** by Michel Olvera, Paraskevas Stamatiadis and Slim Essid.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The experiment evaluates different class description methods for zero-shot audio classification using CLAP (Contrastive Language-Audio Pre-training) models. The notebook compares:\n",
    "\n",
    "- **CLS**: Standard class names\n",
    "- **Context**: Contextual descriptions\n",
    "- **Ontology**: Ontology-based descriptions  \n",
    "- **Base**: Basic descriptions\n",
    "- **Dictionary**: Dictionary-style descriptions\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The evaluation uses a cross-validation approach where for each class, the system selects the best-performing description type (CLS vs. definition-based) during training, then applies this mapping to test data. Performance is measured using class-wise accuracy for single-label datasets or mean Average Precision (mAP) for multi-label datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3781f",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49b5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on slurm with job id: 698390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from msclap import CLAP\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from config import conf, common_parameters\n",
    "from pprint import pprint\n",
    "from utilities import merge_dicts\n",
    "from metrics_helper import compute_metrics, compute_class_wise_accuracy\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62c551",
   "metadata": {},
   "source": [
    "# Set experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9959e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': '698390',\n",
       " 'output_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/003_evaluate_prompts/results',\n",
       " 'similarities_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/003_evaluate_prompts/similarities',\n",
       " 'audio_embeddings_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/001_extract_audio_embeddings/embeddings',\n",
       " 'text_embeddings_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/002_extract_text_embeddings/embeddings',\n",
       " 'model_name': 'CLAP-MS-23',\n",
       " 'definition_type': 'CLS',\n",
       " 'test_dataset': 'ESC50',\n",
       " 'evaluation_mode': 'CLS'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_id = \"001\" # CD Methods for ESC50 Dataset\n",
    "# conf_id = \"002\" # CD Methods for FSD50K Dataset\n",
    "\n",
    "conf = merge_dicts(common_parameters, conf[conf_id])\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be58fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise model selection through cross-validation\n",
    "\n",
    "model_name = conf['model_name']\n",
    "test_dataset = conf['test_dataset']\n",
    "definition_type = conf['definition_type']\n",
    "model_name = conf['model_name']\n",
    "evaluation_mode = conf['evaluation_mode']\n",
    "\n",
    "# Load dataset\n",
    "audio_embeddings_path = os.path.join(conf['audio_embeddings_folder'],\n",
    "                                        model_name,\n",
    "                                        test_dataset + '.pt')\n",
    "\n",
    "if test_dataset != 'TUT2017':\n",
    "    definition_types = ['CLS', 'context', 'ontology', 'base', 'dictionary']\n",
    "else:\n",
    "    definition_types = ['CLS', 'ontology', 'base', 'dictionary']\n",
    "\n",
    "\n",
    "# Load text embeddings for each definition type\n",
    "text_embeddings_paths = []\n",
    "for definition_type in definition_types:\n",
    "    text_embeddings_paths.append(os.path.join(conf['text_embeddings_folder'],\n",
    "                                        'CLAP-MS-23',\n",
    "                                        test_dataset + '_' + definition_type + '.pkl'))\n",
    "    \n",
    "# Load CLAP model\n",
    "if model_name == 'CLAP-MS-23':\n",
    "    clap_model = CLAP(version = '2023', use_cuda=True)\n",
    "else:\n",
    "    raise ValueError('Please specify a valid model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb6f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape:  torch.Size([2000, 1024])\n",
      "Labels shape:  torch.Size([2000, 50])\n"
     ]
    }
   ],
   "source": [
    "# Read embeddings\n",
    "audio_embeddings = torch.load(audio_embeddings_path)\n",
    "print(\"Audio embeddings shape: \", audio_embeddings.shape)\n",
    "\n",
    "# Read ground-truth labels\n",
    "labels = torch.load(audio_embeddings_path.replace('.pt', '_labels.pt'))\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "# Labels are one-hot encoded. Convert them to integers\n",
    "if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "    labels_1D = labels.detach().cpu().numpy()\n",
    "else:\n",
    "    labels_1D = torch.argmax(labels, dim=1) \n",
    "\n",
    "# Read text embeddings dictionaries\n",
    "prompts_dictionary_list = []\n",
    "for text_embeddings_path in text_embeddings_paths:\n",
    "    with open(text_embeddings_path, 'rb') as f:\n",
    "        prompts_dictionary_list.append(pickle.load(f))\n",
    "\n",
    "# Select the text embeddings from the key '' in the dictionaries\n",
    "text_embeddings_list = []\n",
    "for prompts_dictionary in prompts_dictionary_list:\n",
    "    text_embeddings_list.append(prompts_dictionary['']['embeddings'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21bef2",
   "metadata": {},
   "source": [
    "## Cross-validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8b5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape:  torch.Size([2000, 1024])\n",
      "Labels shape:  torch.Size([2000, 50])\n",
      "Text embeddings torch.Size([50, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation. Generate deterministic folds of audio embeddings, text embeddings and labels\n",
    "# using scikit-learn's StratifiedKFold\n",
    "\n",
    "# Print shape of data\n",
    "print(\"Audio embeddings shape: \", audio_embeddings.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "print(\"Text embeddings\", text_embeddings_list[0].shape)\n",
    "\n",
    "audio_embeddings_train_folds = []\n",
    "labels_train_folds = []\n",
    "\n",
    "audio_embeddings_test_folds = []\n",
    "labels_test_folds = []\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "seed = 1200\n",
    "\n",
    "# Define the stratified k-fold object\n",
    "if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "    # Cross validation suitable for multi-label classification\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "else:\n",
    "    # Cross validation suitable for single-label classification\n",
    "    # StratifiedKFold is used to ensure that the proportion of classes is the same in each fold\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "# Generate the folds\n",
    "for train_index, test_index in kf.split(audio_embeddings, labels_1D):\n",
    "\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    audio_embeddings_train_folds.append(audio_embeddings[train_index])\n",
    "    labels_train_folds.append(labels[train_index])\n",
    "\n",
    "    audio_embeddings_test_folds.append(audio_embeddings[test_index])\n",
    "    labels_test_folds.append(labels[test_index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58a6f9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81210caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold:  0\n",
      "Training accuracy: 0.973125\n",
      "Test Accuracy: 0.9075\n",
      "Oracle test accuracy: 0.985\n",
      "------------------------------\n",
      "\n",
      "Fold:  1\n",
      "Training accuracy: 0.9775\n",
      "Test Accuracy: 0.885\n",
      "Oracle test accuracy: 0.975\n",
      "------------------------------\n",
      "\n",
      "Fold:  2\n",
      "Training accuracy: 0.975\n",
      "Test Accuracy: 0.895\n",
      "Oracle test accuracy: 0.9825\n",
      "------------------------------\n",
      "\n",
      "Fold:  3\n",
      "Training accuracy: 0.975625\n",
      "Test Accuracy: 0.885\n",
      "Oracle test accuracy: 0.98\n",
      "------------------------------\n",
      "\n",
      "Fold:  4\n",
      "Training accuracy: 0.973125\n",
      "Test Accuracy: 0.9025\n",
      "Oracle test accuracy: 0.99\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "if evaluation_mode == 'CLS':\n",
    "\n",
    "    # We'll take the max accuracy/mAP between CLS and the other definition types\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(\"\\nFold: \", fold)\n",
    "\n",
    "        metric_results = [] # List to store the dictionary of results of each definition type\n",
    "\n",
    "        for text_embeddings in text_embeddings_list:\n",
    "            # Compute similarity\n",
    "            y_labels = labels_train_folds[fold].detach().cpu().numpy()\n",
    "            similarity = clap_model.compute_similarity(audio_embeddings_train_folds[fold], text_embeddings)\n",
    "\n",
    "            if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "                # Process similarities and compute mAP\n",
    "                y_pred = similarity.detach().cpu().numpy()\n",
    "\n",
    "                _, _, class_wise_metrics_dict = compute_metrics(y_labels, y_pred, normalize_scores=False)\n",
    "                # metric_result = sum(ap.values())/len(ap.values())\n",
    "                # print('mAP: {}'.format(metric_result))\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Process similarities and compute accuracy\n",
    "                y_pred = F.softmax(similarity, dim=1).detach().cpu().numpy()\n",
    "                \n",
    "                class_wise_metrics_dict = compute_class_wise_accuracy(\n",
    "                    np.argmax(y_labels, axis=1),\n",
    "                    np.argmax(y_pred, axis=1))\n",
    "                \n",
    "                # print('Accuracy: {}'.format(metric_result))\n",
    "            \n",
    "            metric_results.append(class_wise_metrics_dict)\n",
    "\n",
    "\n",
    "        # # Take the max accuracy/mAP between CLS and the other definition types\n",
    "        CLS_metrics_dict = metric_results[0]\n",
    "        definitions_metrics_dicts = metric_results[1:]\n",
    "\n",
    "        # print lenghts\n",
    "        # print(\"CLS_metrics_dict: \", len(CLS_metrics_dict)) # 1 x50\n",
    "        # print(\"definitions_metrics_dict: \", len(definitions_metrics_dicts)) # 4 x 50\n",
    "\n",
    "\n",
    "        # Model-class mapping\n",
    "        mapping = dict() \n",
    "        metrics_dict = CLS_metrics_dict.copy()\n",
    "\n",
    "        # CD-ALL configuration. An ensemble among all definition types\n",
    "        # The ensemble is composed of the best predictor for each class among the definition types\n",
    "        for class_index in CLS_metrics_dict.keys():\n",
    "            def_index = 0\n",
    "            for i, definition_dict in enumerate(definitions_metrics_dicts):\n",
    "                if definition_dict[class_index] > metrics_dict[class_index]:\n",
    "                    metrics_dict[class_index] = definition_dict[class_index]\n",
    "                    def_index = i+1\n",
    "                else:\n",
    "                    def_index = def_index\n",
    "            mapping[class_index] = def_index\n",
    "\n",
    "        # Print mapping\n",
    "        # print(\"Mapping: \", mapping)\n",
    "                    \n",
    "        metric_result_training = sum(metrics_dict.values()) / len(metrics_dict.values())\n",
    "        print('Training accuracy: {}'.format(metric_result_training))\n",
    "\n",
    "\n",
    "        test_cval = True\n",
    "\n",
    "        if test_cval:\n",
    "            # Test the mapping in the test set of the fold\n",
    "            metric_results = [] # List to store the dictionary of results of each definition type\n",
    "\n",
    "\n",
    "            for text_embeddings in text_embeddings_list:\n",
    "                # Compute similarity\n",
    "                y_labels = labels_test_folds[fold].detach().cpu().numpy()\n",
    "\n",
    "                similarity = clap_model.compute_similarity(audio_embeddings_test_folds[fold], text_embeddings)\n",
    "\n",
    "                if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "                    # Process similarities and compute mAP\n",
    "                    # y_pred = F.sigmoid(similarity).detach().cpu().numpy()\n",
    "                    y_pred = similarity.detach().cpu().numpy()\n",
    "\n",
    "                    _, _, class_wise_metrics_dict = compute_metrics(y_labels, y_pred, normalize_scores=False)\n",
    "                    # metric_result = sum(ap.values())/len(ap.values())\n",
    "                    # print('mAP: {}'.format(metric_result))\n",
    "                \n",
    "                else:\n",
    "\n",
    "                    # Process similarities and compute accuracy\n",
    "                    y_pred = F.softmax(similarity, dim=1).detach().cpu().numpy()\n",
    "                    \n",
    "                    class_wise_metrics_dict = compute_class_wise_accuracy(\n",
    "                        np.argmax(y_labels, axis=1),\n",
    "                        np.argmax(y_pred, axis=1))\n",
    "                    \n",
    "                    # print('Accuracy: {}'.format(metric_result))\n",
    "                \n",
    "                metric_results.append(class_wise_metrics_dict)\n",
    "\n",
    "\n",
    "            # # Take the max accuracy/mAP between CLS and the other definition types\n",
    "            CLS_metrics_dict = metric_results[0]\n",
    "            definitions_metrics_dicts = metric_results[1:]\n",
    "\n",
    "            metrics_dict = CLS_metrics_dict.copy()\n",
    "            for j, definition_dict in enumerate(definitions_metrics_dicts):\n",
    "                # Definition type index\n",
    "                metrics_dict = CLS_metrics_dict.copy()\n",
    "                for class_index in definition_dict.keys():\n",
    "                    # Use the mapping to select the best predictor for each class\n",
    "                    if mapping[class_index] == j+1:\n",
    "                        # print(\"Class index: definition\", class_index, j+1)\n",
    "                        metrics_dict[class_index] = definition_dict[class_index]\n",
    "                    \n",
    "\n",
    "            metric_result_test = sum(metrics_dict.values()) / len(metrics_dict.values())\n",
    "            print('Test Accuracy: {}'.format(metric_result_test))\n",
    "\n",
    "\n",
    "\n",
    "            # Model-class mapping Oracle test (upper bound)\n",
    "            mapping = dict() \n",
    "            metrics_dict = CLS_metrics_dict.copy()\n",
    "\n",
    "            # Oracle configuration:This represents the theoretical maximum performance achievable with perfect hindsight about which description works best per class. \n",
    "            # The gap between actual test accuracy (using training-based selection) and oracle accuracy reveals potential room for improvement in the model selection strategy.\n",
    "            # The oracle ensemble is composed of the best predictor for each class among the definition types\n",
    "            for class_index in CLS_metrics_dict.keys():\n",
    "                def_index = 0\n",
    "                for i, definition_dict in enumerate(definitions_metrics_dicts):\n",
    "                    if definition_dict[class_index] > metrics_dict[class_index]:\n",
    "                        metrics_dict[class_index] = definition_dict[class_index]\n",
    "                        def_index = i+1\n",
    "                    else:\n",
    "                        def_index = def_index\n",
    "                mapping[class_index] = def_index\n",
    "\n",
    "            # Print the oracle mapping\n",
    "            # print(\"Oracle Mapping: \", mapping)\n",
    "                        \n",
    "            metric_result_oracle_test = sum(metrics_dict.values()) / len(metrics_dict.values())\n",
    "            print('Oracle test accuracy: {}'.format(metric_result_oracle_test))\n",
    "            print(\"---\"*10)\n",
    "\n",
    "\n",
    "            # Compose a dataframe with results and save it to a CSV file\n",
    "            # Columns: model_name, test_dataset, fold, definition_type, training_result, test_result\n",
    "            results = pd.DataFrame({\n",
    "                'model_name': [model_name],\n",
    "                'test_dataset': [test_dataset],\n",
    "                'fold': [fold],\n",
    "                'training_result': [metric_result_training],\n",
    "                'test_result': [metric_result_test],\n",
    "                'oracle_test_result': [metric_result_oracle_test]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Please specify a valid evaluation mode')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27c4f6",
   "metadata": {},
   "source": [
    "# Compute final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c7ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Definition type:  dictionary\n",
      "Training result: 0.9749 +/- 0.0018\n",
      "Test result: 0.8950 +/- 0.0102\n",
      "Oracle Test result: 0.9825 +/- 0.0056\n"
     ]
    }
   ],
   "source": [
    "# Compute final results across folds from results dataframe\n",
    "\n",
    "# Compute mean and std of training and test results across folds\n",
    "training_mean = results_df['training_result'].mean()\n",
    "training_std = results_df['training_result'].std()\n",
    "\n",
    "test_mean = results_df['test_result'].mean()\n",
    "test_std = results_df['test_result'].std()\n",
    "\n",
    "test_oracle_mean = results_df['oracle_test_result'].mean()\n",
    "test_oracle_std = results_df['oracle_test_result'].std()\n",
    "\n",
    "print(\"\\nDefinition type: \", definition_type)\n",
    "print(\"Training result: {:.4f} +/- {:.4f}\".format(training_mean, training_std))\n",
    "print(\"Test result: {:.4f} +/- {:.4f}\".format(test_mean, test_std))\n",
    "print(\"Oracle Test result: {:.4f} +/- {:.4f}\".format(test_oracle_mean, test_oracle_std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
