{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cec52a7",
   "metadata": {},
   "source": [
    "**(CD Methods for FSD50K dataset)**\n",
    "# Audio Classification with Sound Descriptions\n",
    "\n",
    "This notebook reproduces results for the ESC-50 and FSD50K datasets shown in the paper **\"A SOUND DESCRIPTION: EXPLORING PROMPT TEMPLATES AND CLASS DESCRIPTIONS TO ENHANCE ZERO-SHOT AUDIO CLASSIFICATION\"** by Michel Olvera, Paraskevas Stamatiadis and Slim Essid.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The experiment evaluates different class description methods for zero-shot audio classification using CLAP (Contrastive Language-Audio Pre-training) models. The notebook compares:\n",
    "\n",
    "- **CLS**: Standard class names\n",
    "- **Context**: Contextual descriptions\n",
    "- **Ontology**: Ontology-based descriptions  \n",
    "- **Base**: Basic descriptions\n",
    "- **Dictionary**: Dictionary-style descriptions\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The evaluation uses a cross-validation approach where for each class, the system selects the best-performing description type (CLS vs. definition-based) during training, then applies this mapping to test data. Performance is measured using class-wise accuracy for single-label datasets or mean Average Precision (mAP) for multi-label datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a3781f",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f49b5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from msclap import CLAP\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from config import conf, common_parameters\n",
    "from pprint import pprint\n",
    "from utilities import merge_dicts\n",
    "from metrics_helper import compute_metrics, compute_class_wise_accuracy\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62c551",
   "metadata": {},
   "source": [
    "# Set experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9959e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': '698390',\n",
       " 'output_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/003_evaluate_prompts/results',\n",
       " 'similarities_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/003_evaluate_prompts/similarities',\n",
       " 'audio_embeddings_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/001_extract_audio_embeddings/embeddings',\n",
       " 'text_embeddings_folder': '/tsi/audiosig/audible/dcase/studies/016_CLAP_prompting_with_descriptors/002_extract_text_embeddings/embeddings',\n",
       " 'model_name': 'CLAP-MS-23',\n",
       " 'definition_type': 'CLS',\n",
       " 'test_dataset': 'FSD50K',\n",
       " 'evaluation_mode': 'CLS'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf_id = \"001\" # CD Methods for ESC50 Dataset\n",
    "conf_id = \"002\" # CD Methods for FSD50K Dataset\n",
    "\n",
    "conf = merge_dicts(common_parameters, conf[conf_id])\n",
    "conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-wise model selection through cross-validation\n",
    "\n",
    "model_name = conf['model_name']\n",
    "test_dataset = conf['test_dataset']\n",
    "definition_type = conf['definition_type']\n",
    "model_name = conf['model_name']\n",
    "evaluation_mode = conf['evaluation_mode']\n",
    "\n",
    "# Load dataset\n",
    "audio_embeddings_path = os.path.join(conf['audio_embeddings_folder'],\n",
    "                                        model_name,\n",
    "                                        test_dataset + '.pt')\n",
    "\n",
    "if test_dataset != 'TUT2017':\n",
    "    definition_types = ['CLS', 'context', 'ontology', 'base', 'dictionary']\n",
    "else:\n",
    "    definition_types = ['CLS', 'ontology', 'base', 'dictionary']\n",
    "\n",
    "\n",
    "# Load text embeddings for each definition type\n",
    "text_embeddings_paths = []\n",
    "for definition_type in definition_types:\n",
    "    text_embeddings_paths.append(os.path.join(conf['text_embeddings_folder'],\n",
    "                                        'CLAP-MS-23',\n",
    "                                        test_dataset + '_' + definition_type + '.pkl'))\n",
    "    \n",
    "# Load CLAP model\n",
    "if model_name == 'CLAP-MS-23':\n",
    "    clap_model = CLAP(version = '2023', use_cuda=True)\n",
    "else:\n",
    "    raise ValueError('Please specify a valid model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2eb6f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape:  torch.Size([51197, 1024])\n",
      "Labels shape:  torch.Size([51197, 200])\n"
     ]
    }
   ],
   "source": [
    "# Read embeddings\n",
    "audio_embeddings = torch.load(audio_embeddings_path)\n",
    "print(\"Audio embeddings shape: \", audio_embeddings.shape)\n",
    "\n",
    "# Read ground-truth labels\n",
    "labels = torch.load(audio_embeddings_path.replace('.pt', '_labels.pt'))\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "\n",
    "# Labels are one-hot encoded. Convert them to integers\n",
    "if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "    labels_1D = labels.detach().cpu().numpy()\n",
    "else:\n",
    "    labels_1D = torch.argmax(labels, dim=1) \n",
    "\n",
    "# Read text embeddings dictionaries\n",
    "prompts_dictionary_list = []\n",
    "for text_embeddings_path in text_embeddings_paths:\n",
    "    with open(text_embeddings_path, 'rb') as f:\n",
    "        prompts_dictionary_list.append(pickle.load(f))\n",
    "\n",
    "# Select the text embeddings from the key '' in the dictionaries\n",
    "text_embeddings_list = []\n",
    "for prompts_dictionary in prompts_dictionary_list:\n",
    "    text_embeddings_list.append(prompts_dictionary['']['embeddings'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21bef2",
   "metadata": {},
   "source": [
    "## Cross-validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ed8b5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio embeddings shape:  torch.Size([51197, 1024])\n",
      "Labels shape:  torch.Size([51197, 200])\n",
      "Text embeddings torch.Size([200, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation. Generate deterministic folds of audio embeddings, text embeddings and labels\n",
    "# using scikit-learn's StratifiedKFold\n",
    "\n",
    "# Print shape of data\n",
    "print(\"Audio embeddings shape: \", audio_embeddings.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "print(\"Text embeddings\", text_embeddings_list[0].shape)\n",
    "\n",
    "audio_embeddings_train_folds = []\n",
    "labels_train_folds = []\n",
    "\n",
    "audio_embeddings_test_folds = []\n",
    "labels_test_folds = []\n",
    "\n",
    "# Define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Define the seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Define the stratified k-fold object\n",
    "if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "    # Cross validation suitable for multi-label classification\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "else:\n",
    "    # Cross validation suitable for single-label classification\n",
    "    # StratifiedKFold is used to ensure that the proportion of classes is the same in each fold\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "# Generate the folds\n",
    "for train_index, test_index in kf.split(audio_embeddings, labels_1D):\n",
    "\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "    audio_embeddings_train_folds.append(audio_embeddings[train_index])\n",
    "    labels_train_folds.append(labels[train_index])\n",
    "\n",
    "    audio_embeddings_test_folds.append(audio_embeddings[test_index])\n",
    "    labels_test_folds.append(labels[test_index])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58a6f9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81210caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold:  0\n",
      "\n",
      "Definition type:  context\n",
      "Training accuracy: 0.5084328638249755\n",
      "Test Accuracy: 0.5093378990053359\n",
      "------------------------------\n",
      "\n",
      "Definition type:  ontology\n",
      "Training accuracy: 0.509634043440286\n",
      "Test Accuracy: 0.5010220721930971\n",
      "------------------------------\n",
      "\n",
      "Definition type:  base\n",
      "Training accuracy: 0.5096588704333738\n",
      "Test Accuracy: 0.4974945917069452\n",
      "------------------------------\n",
      "\n",
      "Definition type:  dictionary\n",
      "Training accuracy: 0.4994131406595965\n",
      "Test Accuracy: 0.4919644601663952\n",
      "------------------------------\n",
      "\n",
      "Fold:  1\n",
      "\n",
      "Definition type:  context\n",
      "Training accuracy: 0.5061936609866228\n",
      "Test Accuracy: 0.5181978362066106\n",
      "------------------------------\n",
      "\n",
      "Definition type:  ontology\n",
      "Training accuracy: 0.5138439861445\n",
      "Test Accuracy: 0.5122187561968957\n",
      "------------------------------\n",
      "\n",
      "Definition type:  base\n",
      "Training accuracy: 0.5131422476210318\n",
      "Test Accuracy: 0.5065613403609018\n",
      "------------------------------\n",
      "\n",
      "Definition type:  dictionary\n",
      "Training accuracy: 0.5044025527028975\n",
      "Test Accuracy: 0.49901809740413106\n",
      "------------------------------\n",
      "\n",
      "Fold:  2\n",
      "\n",
      "Definition type:  context\n",
      "Training accuracy: 0.5073277995963626\n",
      "Test Accuracy: 0.5127104961232662\n",
      "------------------------------\n",
      "\n",
      "Definition type:  ontology\n",
      "Training accuracy: 0.5142176678288085\n",
      "Test Accuracy: 0.5065456883058745\n",
      "------------------------------\n",
      "\n",
      "Definition type:  base\n",
      "Training accuracy: 0.5086305067417345\n",
      "Test Accuracy: 0.5035662578473832\n",
      "------------------------------\n",
      "\n",
      "Definition type:  dictionary\n",
      "Training accuracy: 0.5006162525860424\n",
      "Test Accuracy: 0.4953890959344342\n",
      "------------------------------\n",
      "\n",
      "Fold:  3\n",
      "\n",
      "Definition type:  context\n",
      "Training accuracy: 0.5071859207833229\n",
      "Test Accuracy: 0.5119287912905619\n",
      "------------------------------\n",
      "\n",
      "Definition type:  ontology\n",
      "Training accuracy: 0.5125137042068263\n",
      "Test Accuracy: 0.5079680762207043\n",
      "------------------------------\n",
      "\n",
      "Definition type:  base\n",
      "Training accuracy: 0.5089462161652331\n",
      "Test Accuracy: 0.5066716594530586\n",
      "------------------------------\n",
      "\n",
      "Definition type:  dictionary\n",
      "Training accuracy: 0.5013588954740811\n",
      "Test Accuracy: 0.499515302035506\n",
      "------------------------------\n",
      "\n",
      "Fold:  4\n",
      "\n",
      "Definition type:  context\n",
      "Training accuracy: 0.5079930920537457\n",
      "Test Accuracy: 0.5119418470799361\n",
      "------------------------------\n",
      "\n",
      "Definition type:  ontology\n",
      "Training accuracy: 0.513861219739163\n",
      "Test Accuracy: 0.5094527907098622\n",
      "------------------------------\n",
      "\n",
      "Definition type:  base\n",
      "Training accuracy: 0.5107289402666239\n",
      "Test Accuracy: 0.5052732182140369\n",
      "------------------------------\n",
      "\n",
      "Definition type:  dictionary\n",
      "Training accuracy: 0.5030366055918541\n",
      "Test Accuracy: 0.4998759900878884\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "if evaluation_mode == 'CLS':\n",
    "\n",
    "    # We'll take the max accuracy/mAP between CLS and the other definition types\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(\"\\nFold: \", fold)\n",
    "\n",
    "        metric_results = [] # List to store the dictionary of results of each definition type\n",
    "\n",
    "        for text_embeddings in text_embeddings_list:\n",
    "            # print(\"Text embeddings shape: \", text_embeddings.shape)\n",
    "            # Compute similarity\n",
    "            y_labels = labels_train_folds[fold].detach().cpu().numpy()\n",
    "            similarity = clap_model.compute_similarity(audio_embeddings_train_folds[fold], text_embeddings)\n",
    "\n",
    "            if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "                # Process similarities and compute mAP\n",
    "                y_pred = similarity.detach().cpu().numpy()\n",
    "\n",
    "                _, _, class_wise_metrics_dict = compute_metrics(y_labels, y_pred, normalize_scores=False)\n",
    "                # metric_result = sum(ap.values())/len(ap.values())\n",
    "                # print('mAP: {}'.format(metric_result))\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Process similarities and compute accuracy\n",
    "                y_pred = F.softmax(similarity, dim=1).detach().cpu().numpy()\n",
    "                \n",
    "                class_wise_metrics_dict = compute_class_wise_accuracy(\n",
    "                    np.argmax(y_labels, axis=1),\n",
    "                    np.argmax(y_pred, axis=1))\n",
    "                \n",
    "                # print('Accuracy: {}'.format(metric_result))\n",
    "            \n",
    "            metric_results.append(class_wise_metrics_dict)\n",
    "\n",
    "\n",
    "        # # Take the max accuracy/mAP between CLS and the other definition types\n",
    "        CLS_metrics_dict = metric_results[0]\n",
    "        definitions_metrics_dicts = metric_results[1:]\n",
    "\n",
    "        # Model-class mapping\n",
    "        mapping = dict() # Will store which predictor performs best for each class\n",
    "\n",
    "        for i, definition_dict in enumerate(definitions_metrics_dicts):\n",
    "            # Definition type index\n",
    "            print(\"\\nDefinition type: \", definition_types[i+1])\n",
    "            metrics_dict = CLS_metrics_dict.copy()\n",
    "            for class_index in definition_dict.keys():\n",
    "                initial_accuracy = CLS_metrics_dict[class_index]\n",
    "                new_accuracy = 0\n",
    "                if definition_dict[class_index] > CLS_metrics_dict[class_index]:\n",
    "                    metrics_dict[class_index] = definition_dict[class_index]\n",
    "                    new_accuracy = definition_dict[class_index]\n",
    "                # Construct mapping\n",
    "                if initial_accuracy < new_accuracy:\n",
    "                    mapping[class_index] = 'DEF' # If definition performs better\n",
    "                else:\n",
    "                    mapping[class_index] = 'CLS' # If CLS performs better\n",
    "\n",
    "            # print(\"Mapping: \", mapping)\n",
    "\n",
    "                    \n",
    "            metric_result_training = sum(metrics_dict.values()) / len(metrics_dict.values())\n",
    "            print('Training accuracy: {}'.format(metric_result_training))\n",
    "\n",
    "\n",
    "            test_cval = True\n",
    "\n",
    "            if test_cval:\n",
    "                # Test the mapping in the test set of the fold\n",
    "                metric_results = [] # List to store the dictionary of results of each definition type\n",
    "\n",
    "                text_embeddings_test_list = [text_embeddings_list[0], text_embeddings_list[i+1]]\n",
    "\n",
    "                for text_embeddings in text_embeddings_test_list:\n",
    "                    # print(\"Text embeddings shape: \", text_embeddings.shape)\n",
    "                    # Compute similarity\n",
    "                    y_labels = labels_test_folds[fold].detach().cpu().numpy()\n",
    "\n",
    "                    similarity = clap_model.compute_similarity(audio_embeddings_test_folds[fold], text_embeddings)\n",
    "\n",
    "                    if test_dataset == 'FSD50K' or test_dataset == 'AudioSet' or test_dataset == 'DCASE2017':\n",
    "                        # Process similarities and compute mAP\n",
    "                        y_pred = similarity.detach().cpu().numpy()\n",
    "\n",
    "                        _, _, class_wise_metrics_dict = compute_metrics(y_labels, y_pred, normalize_scores=False)\n",
    "                    \n",
    "                    else:\n",
    "\n",
    "                        # Process similarities and compute accuracy\n",
    "                        y_pred = F.softmax(similarity, dim=1).detach().cpu().numpy()\n",
    "                        \n",
    "                        class_wise_metrics_dict = compute_class_wise_accuracy(\n",
    "                            np.argmax(y_labels, axis=1),\n",
    "                            np.argmax(y_pred, axis=1))\n",
    "                        \n",
    "                        # print('Accuracy: {}'.format(metric_result))\n",
    "                    \n",
    "                    metric_results.append(class_wise_metrics_dict)\n",
    "\n",
    "\n",
    "                # # Take the max accuracy/mAP between CLS and the other definition types\n",
    "                CLS_metrics_dict = metric_results[0]\n",
    "                definitions_metrics_dicts = metric_results[1:]\n",
    "\n",
    "\n",
    "                for definition_dict in definitions_metrics_dicts:\n",
    "                    # Definition type index\n",
    "                    metrics_dict = CLS_metrics_dict.copy()\n",
    "                    for class_index in definition_dict.keys():\n",
    "                        # Use the mapping to select the best predictor for each class\n",
    "                        if mapping[class_index] == 'DEF':\n",
    "                            metrics_dict[class_index] = definition_dict[class_index]\n",
    "\n",
    "                    metric_result_test = sum(metrics_dict.values()) / len(metrics_dict.values())\n",
    "                    print('Test Accuracy: {}'.format(metric_result_test))\n",
    "                    print('---'*10)\n",
    "\n",
    "                    # Store the results in a dictionary\n",
    "                    results = pd.DataFrame({\n",
    "                        'model_name': [model_name],\n",
    "                        'test_dataset': [test_dataset],\n",
    "                        'fold': [fold],\n",
    "                        'definition_type': [definition_types[i+1]],\n",
    "                        'training_result': [metric_result_training],\n",
    "                        'test_result': [metric_result_test]\n",
    "                    })\n",
    "                    results_df = pd.concat([results_df, results], ignore_index=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27c4f6",
   "metadata": {},
   "source": [
    "# Compute final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3c7ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Definition type:  dictionary\n",
      "Training result: 0.5018 +/- 0.0020\n",
      "Test result: 0.4972 +/- 0.0034\n",
      "\n",
      "Definition type:  base\n",
      "Training result: 0.5102 +/- 0.0018\n",
      "Test result: 0.5039 +/- 0.0038\n",
      "\n",
      "Definition type:  context\n",
      "Training result: 0.5074 +/- 0.0009\n",
      "Test result: 0.5128 +/- 0.0033\n",
      "\n",
      "Definition type:  ontology\n",
      "Training result: 0.5128 +/- 0.0019\n",
      "Test result: 0.5074 +/- 0.0042\n"
     ]
    }
   ],
   "source": [
    "# Compute final results across folds from results dataframe\n",
    "definition_types_as_in_paper = ['CLS', 'dictionary', 'base', 'context', 'ontology']\n",
    "\n",
    "for definition_type in definition_types_as_in_paper[1:]:\n",
    "\n",
    "    # Filter results for the current definition type\n",
    "    results_df_def = results_df[results_df['definition_type'] == definition_type]\n",
    "\n",
    "    # Compute mean and std of training and test results across folds\n",
    "    training_mean = results_df_def['training_result'].mean()\n",
    "    training_std = results_df_def['training_result'].std()\n",
    "\n",
    "    test_mean = results_df_def['test_result'].mean()\n",
    "    test_std = results_df_def['test_result'].std()\n",
    "\n",
    "    print(\"\\nDefinition type: \", definition_type)\n",
    "    print(\"Training result: {:.4f} +/- {:.4f}\".format(training_mean, training_std))\n",
    "    print(\"Test result: {:.4f} +/- {:.4f}\".format(test_mean, test_std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
